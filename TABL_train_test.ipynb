{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-PVsZeWjCiw"
      },
      "source": [
        "### **TABL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVPONVeVw0nh"
      },
      "outputs": [],
      "source": [
        "# load packages\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm \n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNdy1u5zjMaw"
      },
      "source": [
        "### **Data**\n",
        "The dataset in the folder Dataset is the FI-2010 dataset zipped and normalized. \n",
        "\n",
        "As in the original paper I used the firs 7 days to train and to validate, and the rest 3 days to do the the testing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls5u0jngxkjl"
      },
      "outputs": [],
      "source": [
        "# please change the data_path to your local path and download the files you need from the web site of the dataset\n",
        "\n",
        "dec_data = np.loadtxt('/content/drive/MyDrive/Data/Train_Dst_NoAuction_ZScore_CF_7.txt')\n",
        "dec_train = dec_data[:, :int(dec_data.shape[1] * 0.8)]\n",
        "dec_val = dec_data[:, int(dec_data.shape[1] * 0.8):]\n",
        "\n",
        "dec_test1 = np.loadtxt('/content/drive/MyDrive/Data/Test_Dst_NoAuction_ZScore_CF_7.txt')\n",
        "dec_test2 = np.loadtxt('/content/drive/MyDrive/Data/Test_Dst_NoAuction_ZScore_CF_8.txt')\n",
        "dec_test3 = np.loadtxt('/content/drive/MyDrive/Data/Test_Dst_NoAuction_ZScore_CF_9.txt')\n",
        "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
        "\n",
        "h = 2        #if h = 2, than horizon = 50\n",
        "\n",
        "y_train = dec_train[-h, :].flatten()\n",
        "y_val = dec_val[-h, :].flatten()\n",
        "y_test = dec_test[-h, :].flatten()\n",
        "\n",
        "y_train = y_train[9:] - 1\n",
        "y_val = y_val[9:] - 1\n",
        "y_test = y_test[9:] - 1 \n",
        "\n",
        "dec_train = dec_train[:40, :].T\n",
        "dec_val = dec_val[:40, :].T\n",
        "dec_test = dec_test[:40, :].T\n",
        "\n",
        "print(dec_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KspbKX4q8TRM"
      },
      "outputs": [],
      "source": [
        "#computing the weights for a weighted cross entropy loss\n",
        "\n",
        "y = np.concatenate((y_train, y_val, y_test))\n",
        "weights = torch.zeros(3, device=device)\n",
        "for i in range(y.shape[0]):\n",
        "  if (y[i] == 0):\n",
        "    weights[0] += 1\n",
        "  elif (y[i] == 1):\n",
        "    weights[1] += 1\n",
        "  else:\n",
        "    weights[2] += 1\n",
        "weights = weights / y.shape[0]\n",
        "print(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x7PAu1LySOZ"
      },
      "outputs": [],
      "source": [
        "class Dataset(data.Dataset):\n",
        "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
        "    def __init__(self, x, y, num_classes, n, dim):\n",
        "        \"\"\"Initialization\"\"\" \n",
        "        self.num_classes = num_classes\n",
        "        self.dim = dim\n",
        "        self.x = x   \n",
        "        self.y = y\n",
        "        self.n = n\n",
        "\n",
        "        self.length = x.shape[0] - (T/10) -self.dim + 1\n",
        "        print(self.length)\n",
        "\n",
        "        x = torch.from_numpy(x)\n",
        "        self.x = torch.unsqueeze(x, 1)\n",
        "        self.y = torch.from_numpy(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the total number of samples\"\"\"\n",
        "        return int(self.length)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        input = self.x[i:i+self.dim, :]\n",
        "        input = input.permute(1, 2, 0)\n",
        "        input = torch.squeeze(input)\n",
        "\n",
        "        return input, self.y[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ndByE-Ajmq8",
        "outputId": "9b68ee43-4e8d-4483-c284-512fb31797e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50936.0\n",
            "139573.0\n",
            "203786.0\n"
          ]
        }
      ],
      "source": [
        "#Hyperparameters\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 200\n",
        "T = 50   #horizon    \n",
        "lr = 0.01\n",
        "num_classes = 3\n",
        "dim = 10\n",
        "n = 3 \n",
        "\n",
        "\n",
        "dataset_val = Dataset(dec_val, y_val, num_classes, n, dim)\n",
        "dataset_test = Dataset(dec_test, y_test, num_classes, n, dim)\n",
        "dataset_train = Dataset(dec_train, y_train, num_classes, n, dim)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEIIi2NwjtgC"
      },
      "source": [
        "### **Model Architecture**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhA2p2kcUj2q"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class TABL_layer(nn.Module):\n",
        "    def __init__(self, d2, d1, t1, t2):\n",
        "        super().__init__()\n",
        "        self.t1 = t1\n",
        "\n",
        "        weight = torch.Tensor(d2, d1)\n",
        "        self.W1 = nn.Parameter(weight)\n",
        "        nn.init.kaiming_uniform_(self.W1, nonlinearity='relu')\n",
        "        \n",
        "        weight2 = torch.Tensor(t1, t1)\n",
        "        self.W = nn.Parameter(weight2)\n",
        "        nn.init.constant_(self.W, 1/t1)\n",
        " \n",
        "        weight3 = torch.Tensor(t1, t2)\n",
        "        self.W2 = nn.Parameter(weight3)\n",
        "        nn.init.kaiming_uniform_(self.W2, nonlinearity='relu')\n",
        "\n",
        "        bias1 = torch.Tensor(d2, t2)\n",
        "        self.B = nn.Parameter(bias1)\n",
        "        nn.init.constant_(self.B, 0)\n",
        "\n",
        "        l = torch.Tensor(1,)\n",
        "        self.l = nn.Parameter(l)\n",
        "        nn.init.constant_(self.l, 0.5)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        \n",
        "        #maintaining the weight parameter between 0 and 1.\n",
        "        if (self.l[0] < 0): \n",
        "          l = torch.Tensor(1,)\n",
        "          self.l = nn.Parameter(l)\n",
        "          nn.init.constant_(self.l, 0.0)\n",
        "\n",
        "        if (self.l[0] > 1): \n",
        "          l = torch.Tensor(1,)\n",
        "          self.l = nn.Parameter(l)\n",
        "          nn.init.constant_(self.l, 1.0)\n",
        "     \n",
        "        #modelling the dependence along the first mode of X while keeping the temporal order intact (7)\n",
        "        X = self.W1 @ X\n",
        "\n",
        "        #enforcing constant (1) on the diagonal\n",
        "        W = self.W -self.W *torch.eye(self.t1,dtype=torch.float32).to(device)+torch.eye(self.t1,dtype=torch.float32).to(device)/self.t1\n",
        "\n",
        "        #attention, the aim of the second step is to learn how important the temporal instances are to each other (8)\n",
        "        E = X @ W\n",
        "\n",
        "        #computing the attention mask  (9)\n",
        "        A = torch.softmax(E, dim=-1)\n",
        "\n",
        "        #applying a soft attention mechanism  (10)\n",
        "        #he attention mask A obtained from the third step is used to zero out the effect of unimportant elements\n",
        "        X = self.l[0] * (X) + (1.0 - self.l[0])*X*A\n",
        "\n",
        "        #the final step of the proposed layer estimates the temporal mapping W2, after the bias shift (11)\n",
        "        y = X @ self.W2 + self.B\n",
        "        return y\n",
        "\n",
        "class BL_layer(nn.Module):\n",
        "  def __init__(self, d2, d1, t1, t2):\n",
        "        super().__init__()\n",
        "        weight1 = torch.Tensor(d2, d1)\n",
        "        self.W1 = nn.Parameter(weight1)\n",
        "        nn.init.kaiming_uniform_(self.W1, nonlinearity='relu')\n",
        "\n",
        "        weight2 = torch.Tensor(t1, t2)\n",
        "        self.W2 = nn.Parameter(weight2)\n",
        "        nn.init.kaiming_uniform_(self.W2, nonlinearity='relu')\n",
        "\n",
        "        bias1 = torch.zeros((d2, t2))\n",
        "        self.B = nn.Parameter(bias1)\n",
        "        nn.init.constant_(self.B, 0)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.activation(self.W1 @ x @ self.W2 + self.B)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class BTABL(nn.Module):\n",
        "  def __init__(self, d2, d1, t1, t2, d3, t3):\n",
        "    super().__init__()\n",
        "\n",
        "    self.BL = BL_layer(d2, d1, t1, t2)\n",
        "    self.TABL = TABL_layer(d3, d2, t2, t3)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    self.max_norm_(self.BL.W1.data)\n",
        "    self.max_norm_(self.BL.W2.data)\n",
        "    x = self.BL(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    self.max_norm_(self.TABL.W1.data)\n",
        "    self.max_norm_(self.TABL.W.data)\n",
        "    self.max_norm_(self.TABL.W2.data)\n",
        "    x = self.TABL(x)\n",
        "    x = torch.squeeze(x)\n",
        "    x = torch.softmax(x, 1)\n",
        "    return x\n",
        "\n",
        "  def max_norm_(self, w):\n",
        "    with torch.no_grad():\n",
        "      if (torch.linalg.matrix_norm(w) > 10.0):\n",
        "        norm = torch.linalg.matrix_norm(w)\n",
        "        desired = torch.clamp(norm, min=0.0, max=10.0)\n",
        "        w *= (desired / (1e-8 + norm))\n",
        "\n",
        "\n",
        "\n",
        "class CTABL(nn.Module):\n",
        "  def __init__(self, d2, d1, t1, t2, d3, t3, d4, t4):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.BL = BL_layer(d2, d1, t1, t2)\n",
        "    self.BL2 = BL_layer(d3, d2, t2, t3)\n",
        "    self.TABL = TABL_layer(d4, d3, t3, t4)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        " \n",
        "    self.max_norm_(self.BL.W1.data)\n",
        "    self.max_norm_(self.BL.W2.data)\n",
        "    x = self.BL(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    self.max_norm_(self.BL2.W1.data)\n",
        "    self.max_norm_(self.BL2.W2.data)\n",
        "    x = self.BL2(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    self.max_norm_(self.TABL.W1.data)\n",
        "    self.max_norm_(self.TABL.W.data)\n",
        "    self.max_norm_(self.TABL.W2.data)\n",
        "    x = self.TABL(x)\n",
        "    x = torch.squeeze(x)\n",
        "    x = torch.softmax(x, 1)\n",
        "    return x\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bejZgDmCkkHi"
      },
      "source": [
        "### **Model Training**\n",
        "\n",
        "I implemented the second setting of the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_u5esKfTT-S"
      },
      "outputs": [],
      "source": [
        "#Choose between B(TABL) and C(TABL)\n",
        "#model = BTABL(120, 40, 10, 5, 3, 1)\n",
        "model = CTABL(60, 40, 10, 10, 120, 5, 3, 1)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "def batch_gd(model, criterion, optimizer, epochs):\n",
        "    \n",
        "    train_losses = np.zeros(epochs)\n",
        "    test_losses = np.zeros(epochs)\n",
        "    best_test_loss = np.inf\n",
        "    best_test_epoch = 0\n",
        "    SC = [0.005, 0.001, 0.0005, 0.0001]\n",
        "    i = 0\n",
        "    for it in tqdm(range(epochs)):\n",
        "        \n",
        "        model.train()\n",
        "        t0 = datetime.now()\n",
        "        train_loss = []\n",
        "        for inputs, targets in train_loader:\n",
        "            # move data to GPU\n",
        "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            #outputs = torch.squeeze(outputs)\n",
        "            #print(outputs.shape)\n",
        "            #print(targets.shape)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "        # Get train loss and test loss\n",
        "        train_loss = np.mean(train_loss)\n",
        "        \n",
        "        if (train_losses[it-1] <= train_loss and i < 4 and it != 0):\n",
        "              for g in optimizer.param_groups:\n",
        "                g['lr'] = SC[i]\n",
        "              i += 1\n",
        "              print(optimizer)\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = []\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)      \n",
        "            outputs = model(inputs)\n",
        "            #outputs = torch.squeeze(outputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss.append(loss.item())\n",
        "        test_loss = np.mean(test_loss)\n",
        "\n",
        "        # Save losses\n",
        "        train_losses[it] = train_loss\n",
        "        test_losses[it] = test_loss\n",
        "        \n",
        "        #We save the best model\n",
        "        if test_loss < best_test_loss:\n",
        "            torch.save(model, '/best_model_CTABL')\n",
        "            best_test_loss = test_loss\n",
        "            best_test_epoch = it\n",
        "            print('model saved')\n",
        "\n",
        "        dt = datetime.now() - t0\n",
        "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
        "          Validation Loss: {test_loss:.4f}, Duration: {dt}, Best Val Epoch: {best_test_epoch}')\n",
        "        \n",
        "    #torch.save(model, '/content/drive/MyDrive/Output/best_model_translob_FI')\n",
        "    return train_losses, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x9vq-ZAzTb6K"
      },
      "outputs": [],
      "source": [
        "print(\"------- List Hyper Parameters -------\")\n",
        "print(\"epochs   ->   \" + str(epochs))\n",
        "print(\"learningRate   ->   \" + str(lr))\n",
        "print(\"horizon    ->     \" + str(T))\n",
        "print(\"batch size   ->    \" + str(batch_size))\n",
        "print(\"Optimizer   ->    \" + str(optimizer))\n",
        "\n",
        "train_losses, val_losses = batch_gd(model, criterion, optimizer, \n",
        "                                     epochs)\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(val_losses, label='validation loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7CF2CwUkn4G"
      },
      "source": [
        "### **Model Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TFg5d6CzTgWS",
        "outputId": "8d1c2dfd-ecff-4f59-e1dd-45504c121328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test acc: 0.7256\n"
          ]
        }
      ],
      "source": [
        "model = torch.load('/best_model_CTABL')\n",
        "\n",
        "n_correct = 0.\n",
        "n_total = 0.\n",
        "all_targets = []\n",
        "all_predictions = []\n",
        "\n",
        "for inputs, targets in test_loader:\n",
        "    # Move to GPU\n",
        "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    #outputs = torch.squeeze(outputs)\n",
        "    # Get prediction\n",
        "    # torch.max returns both max and argmax\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "    # update counts\n",
        "    n_correct += (predictions == targets).sum().item()\n",
        "    n_total += targets.shape[0]\n",
        "\n",
        "    all_targets.append(targets.cpu().numpy())\n",
        "    all_predictions.append(predictions.cpu().numpy())\n",
        "\n",
        "test_acc = n_correct / n_total\n",
        "print(f\"Test acc: {test_acc:.4f}\")\n",
        "  \n",
        "all_targets = np.concatenate(all_targets)    \n",
        "all_predictions = np.concatenate(all_predictions)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0oOu5lwf6zw0",
        "outputId": "a7f1be3e-e874-4733-cead-ca3651a170cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy_score: 0.7256059553065421\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6779    0.6647    0.6712     38464\n",
            "           1     0.7947    0.7914    0.7931     65997\n",
            "           2     0.6497    0.6686    0.6590     35112\n",
            "\n",
            "    accuracy                         0.7256    139573\n",
            "   macro avg     0.7074    0.7082    0.7078    139573\n",
            "weighted avg     0.7260    0.7256    0.7258    139573\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
        "print(classification_report(all_targets, all_predictions, digits=4))\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "c = confusion_matrix(all_targets, all_predictions, normalize=\"true\")\n",
        "disp = ConfusionMatrixDisplay(c)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
